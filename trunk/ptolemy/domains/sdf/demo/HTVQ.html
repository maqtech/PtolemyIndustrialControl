<!-- $Id$-->
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<link href="../../../../doc/default.css" rel="stylesheet" type="text/css">
<TITLE>Vector Quantization Demo</TITLE>
</HEAD>
<BODY>

<H1>Vector Quantization Demo</H1>

<CENTER>
<P><APPLET name="HTVQApplet"
   bgcolor=#FAF0E6
   code=ptolemy.domains.sdf.demo.HTVQApplet.class
   archive="ptolemy/ptsupport.jar,ptolemy/domains/sdf/sdf.jar"
   codebase=../../../..
   width=420 height=200>
<param name="background" value="#faf0e6">
<param name="iterations" value="100">
<BR><I>If you were able to run applets, you would have a video demo
here.</I>
<BR></APPLET></CENTER>
<p>
This applet simulates a simple video compression scheme based on Vector Quantization (VQ).   The model looks like the following:
<p><br>
<center>
<IMG SRC="HTVQModel.gif" ALT="dataflow model">
</center><br>
Each frame is first subdivided into blocks of 4 pixels by 2 pixels.   Each block is then passed through a Vector Quantizer, which selects the best reconstruction block from a precomputed codebook.   The output of the Vector Quantizer is the codebook index of the reconstruction vector.   Each block is decompressed by indexing into the codebook, and then reassembled into the output image.   <p>
The Vector Quantizer in this implementation does not actually find the optimal reconstruction vector for each block.   Instead, the quantization is done using a series of 3 table lookups.  The resulting quantizer is very close to optimal and results in a significantly faster implementation.   This technique is known as Hierarchical Table-Lookup Vector Quantization (HTVQ).
<p><br>
<CENTER>
<IMG SRC="HTVQSShot.gif" ALT="screen shot">
</CENTER>
<p><font size="2" color="#cc0000">
Last Updated: $Date$
</font>
</body>
</html>






